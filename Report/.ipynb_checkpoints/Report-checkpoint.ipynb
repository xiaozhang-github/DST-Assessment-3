{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Natural Language Processing work focusses on the Enron email data set which contains both spam and real emails for us to analyse. The research we wanted to perform was: *Analyse the Enron spam vs normal data set to create and visualise a topic model and understand the features of spam emails.* We did this by performing pre-processing on the data set, then creating a model and then analysing the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a summary of the results we have found through our research into NLP on the Enron email set. We summarise with the classification report for each model and a data frame containing the perplexity and coherence scores for each optimal model created. More results and visualisations of models can be found within our own folders and should definitely be looked at to understand the analysis that took place since the complexity of that cannot be shown through the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifcation Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def classification_eval(y_true,y_pred):\n",
    "    \n",
    "    print(\"Confusion Matrix\")\n",
    "    C = confusion_matrix(y_true,y_pred)\n",
    "    \n",
    "    print('Classification report')\n",
    "    print(classification_report(y_true, y_pred, target_names = ['Normal', 'Spam'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the predictions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matt's predictions\n",
    "Matt_actual = pickle.load(open('../Data/Actual.p','rb'))\n",
    "Matt_pred = pickle.load(open('../Data/Matt_pred.p','rb'))\n",
    "\n",
    "# Alex predictions\n",
    "Alex_actual = pickle.load(open('../Data/Alex_y_actual.p','rb')) \n",
    "Alex_pred = pickle.load(open('../Data/Alex_y_pred.p','rb'))\n",
    "\n",
    "# Xiao predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal      0.844     0.859     0.851     15046\n",
      "        Spam      0.829     0.811     0.820     12670\n",
      "\n",
      "    accuracy                          0.837     27716\n",
      "   macro avg      0.836     0.835     0.835     27716\n",
      "weighted avg      0.837     0.837     0.837     27716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_eval(Matt_actual,Matt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal      0.608     0.709     0.655     15046\n",
      "        Spam      0.570     0.457     0.507     12670\n",
      "\n",
      "    accuracy                          0.594     27716\n",
      "   macro avg      0.589     0.583     0.581     27716\n",
      "weighted avg      0.590     0.594     0.587     27716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_eval(Alex_actual,Alex_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification_eval(Xiao_actual,Xiao_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity and Coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import our scores for our optimal models here and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matt_values = pickle.load(open('../Data/Matt_opt_values.p','rb'))\n",
    "Matt_values = ['LDA Matt'] + Matt_values\n",
    "Matt_values.pop(2)\n",
    "\n",
    "Alex_values = pickle.load(open('../Data/Alex_optimal_values.p','rb'))\n",
    "Alex_values.reverse()\n",
    "Alex_values = ['LDA_td-idf Alex'] + Alex_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_df = pd.DataFrame(columns=['Model','Perplexity','c_v coherence'])\n",
    "co_df.loc[0] = Matt_values\n",
    "co_df.loc[1] = Alex_values\n",
    "#co_df.loc[2] = Xiao_values # Xiao ended up using an NMF implementation for topic modelling and thus is unable to produce \n",
    "                            # perplexity/coherence scores for comparison to mine and Alex's models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>c_v coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDA Matt</td>\n",
       "      <td>-8.149243</td>\n",
       "      <td>0.561903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA_td-idf Alex</td>\n",
       "      <td>-21.449396</td>\n",
       "      <td>0.480509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Perplexity  c_v coherence\n",
       "0         LDA Matt   -8.149243       0.561903\n",
       "1  LDA_td-idf Alex  -21.449396       0.480509"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with removal of re/fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alex_rf_pred = pickle.load(open('../Data/y_pred_re.p','rb'))\n",
    "Alex_rf_actual = pickle.load(open('../Data/y_actual_re.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal      0.830     0.800     0.815      4736\n",
      "        Spam      0.325     0.371     0.346      1229\n",
      "\n",
      "    accuracy                          0.711      5965\n",
      "   macro avg      0.578     0.585     0.581      5965\n",
      "weighted avg      0.726     0.711     0.718      5965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_eval(Alex_rf_actual,Alex_rf_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
