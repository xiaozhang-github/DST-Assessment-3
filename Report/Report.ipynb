{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Natural Language Processing work focusses on the Enron email data set which contains both spam and real emails for us to analyse. The research we wanted to perform was: *Analyse the Enron spam vs normal data set to create and visualise a topic model and understand the features of spam emails.* We did this by performing pre-processing on the data set, then creating a model and then analysing the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a summary of the results we have found through our research into NLP on the Enron email set. We summarise with the classification report for each model and a data frame containing the perplexity and coherence scores for each optimal model created. More results and visualisations of models can be found within our own folders and should definitely be looked at to understand the analysis that took place since the complexity of that cannot be shown through the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifcation Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def classification_eval(y_true,y_pred):\n",
    "    \n",
    "    print(\"Confusion Matrix\")\n",
    "    C = confusion_matrix(y_true,y_pred)\n",
    "    \n",
    "    print('Classification report')\n",
    "    print(classification_report(y_true, y_pred, target_names = ['Normal', 'Spam'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the predictions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = pickle.load(open('../Data/Actual.p','rb'))\n",
    "Matt_pred = pickle.load(open('../Data/Matt_pred.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal      0.540     0.749     0.628     15046\n",
      "        Spam      0.449     0.243     0.315     12670\n",
      "\n",
      "    accuracy                          0.518     27716\n",
      "   macro avg      0.494     0.496     0.471     27716\n",
      "weighted avg      0.498     0.518     0.485     27716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_eval(actual,Matt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification_eval(actual,Alex_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification_eval(actual,Xiao_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity and Coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import our scores here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matt_values = pickle.load(open('../Data/Matt_opt_values.p','rb'))\n",
    "Matt_values = ['LDA Matt'] + Matt_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_df = pd.DataFrame(columns=['Model','Perplexity','u_mass coherence','c_v coherence'])\n",
    "co_df.loc[0] = Matt_values\n",
    "#co_df.loc[1] = Alex_values\n",
    "#co_df.loc[2] = Xiao_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>u_mass coherence</th>\n",
       "      <th>c_v coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDA Matt</td>\n",
       "      <td>-8.14925</td>\n",
       "      <td>-1.866637</td>\n",
       "      <td>0.561903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Perplexity  u_mass coherence  c_v coherence\n",
       "0  LDA Matt    -8.14925         -1.866637       0.561903"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
