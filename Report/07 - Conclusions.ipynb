{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Our team tackled the task of filtering spam from normal e-mails on a toy example dataset of about 28000 datapoints. To this extent, we may claim that results heavily varied from one implementation to the other of our models, but the overall accuracy of the runs was satisfactory. The reasons identified for these wide variations were mainly the following two:\n",
    "\n",
    "1) _Sampling:_ Due to the nature of our inquiry (i.e. mostly for separating spam for ham so they do not show up as consecutive datapoints) sampling was heavily involved in determining the training and testing sets. Running different seeds will therefore be bound to produce different results.\n",
    "\n",
    "2) _The models themselves:_ The way the models operate may sometimes identify borderline cases as spam and sometimes as normal e-mails. This couldn't be helped regardless of our choice of implementation.\n",
    "\n",
    "We studied 3 different topic models as a means of achieving our desired classification: TF-IDF (by Alex), LDA (by Matt) and NMF (by Xiao).\n",
    "\n",
    "In terms of **accuracy**, the latter model performed best on its optimal solution, achieving a 99.7% score with better performance on normal e-mails - on the whole dataset. LDA and TF-IDF were instead measured on a heavily suboptimal model of only 2 topics, where they achieved results varying from 55% to 89% accuracy, and 55 to 78% respectively. Provided the optimal solution (which we measured, but only implemented visually) and the optimal scores achieved for our two measurements of fitness (perplexity and coherence) - the expected optimal accuracy of LDA and TF-IDF would be ~ 98% and ~ 90% respectively. Still, that leaves NMF as the best performing model, followed by LDA:\n",
    "\n",
    "1. NMF\n",
    "\n",
    "2. LDA\n",
    "\n",
    "3. TF-IDF\n",
    "\n",
    "In terms of **coherence**, NMF surprisingly achieved the lowest score of 0.356. TF-IDF set its best score at 0.48 whereas LDA performed best on this metric with 0.561.\n",
    "\n",
    "1. LDA\n",
    "\n",
    "2. TF-IDF\n",
    "\n",
    "3. NMF\n",
    "\n",
    "In terms of **perplexity**, NMF had no appropriable measurement for this metric. LDA achieved its lowest (and hence, best) score at -8.14 whereas TF-IDF showed a continuously decreasing trend - after testing on 49 topics, achieved its lowest for the greatest number of topics at -21.44.\n",
    "\n",
    "1. TF-IDF\n",
    "\n",
    "2. LDA\n",
    "\n",
    "**Interpreting these results:**\n",
    "\n",
    "NMF operates in a way that doesn't require exceedingly high outputs of coherence score to perform good. LDA and TF-IDF, however, consistently performed better the higher coherence (and lower perplexity) they achieved. Since LDA achieved a better overall coherence, it links into the explanation of why it consistently outperformed TF-IDF (on most seeds, not all). Perplexity, however, although consistently smaller on TF-IDF's side, did not seem to present as much weighted importance to the final score as coherence did. Lastly, the choice of dataset was found to be suitable for all 3 models, with no particular tendency of favoring one over the other (for instance, due to the structure or nature of the data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final reflections\n",
    "\n",
    "**Should we expect this ranking to consistently extend to other datasets/projects?**\n",
    "\n",
    "We believe one example is highly insufficient for drawing such a strong conclusion. Best we can say is that, at least within our implementation, LDA seemed to consistently outperform TF-IDF for the most part; although certain features of TF-IDF were valuable to our progression in themselves, such as the perplexity trend. As for NMF, one study is nowhere near conclusive enough to its performance, especially so since it did not follow the same measurement protocol as the other two (nor had it a perplexity score). Valuable, thought-provoking features were drawn by our team from all 3 models and a proper study into their comparative performances is yet to be made.\n",
    "\n",
    "**Was it a good dataset to test on?**\n",
    "\n",
    "Absolutely. It was especially good for the task at hand, i.e. filtering, since we already knew the results in advance and could obtain access to an accuracy measurement with ease. Moreover, the main reason behind our decisive answer is the dictionary provided: Over 100589 unique words are present in our dictionary (for a portion of 90% of the total dataset, at least) - number checked by one of our contributors [here, at Section 3](https://github.com/xiaozhang-github/DST-Assessment-3/blob/main/Alex%20Caian/Assessment%203.ipynb). This number offers great depth for research and provides plenty of information to our models. A much larger dataset, but _with the same_ dictionary would've allowed for nearly any big data approach to the problem, such as neural networks implementation. Admittedly, a good fraction of the words in our dictionary were nonsense - but this too was valuable to our models for classifying them as likely part of a spam e-mail.\n",
    "\n",
    "**Is there anything else to learn from this work?**\n",
    "\n",
    "Although we believe to have obtained plenty of useful information and insights from this project, areas for further exploration could certainly be found on this example. For one, the 'replies & forwards' analysis could be broadly extended beyond what we did. Separating certain subtypes of e-mails based on intrinsic characteristics they pose, such as this 're&fw' separation could potentially massively improve the performance of one of our models, or perhaps of a 4th one. One way to extend that analysis would be by looking at the dataset with the 're&fw' e-mails removed, and then adding them back in manually, portion by portion, to dynamically observe the models' change in performance. The biggest problem this poses (as well as a naturally recurring problem in our project too!) is the computational complexity of such an inquiry. Either exceedingly good optimization or tremendous computational power are needed to perform this kind of analysis; since our models alone took up to several hours of computing time.\n",
    "\n",
    "## Contributors\n",
    "\n",
    "_Link to contributor's profile ~ Link to their work on this project_\n",
    "\n",
    "[Alex Caian](https://github.com/Alex-Caian) ~ [Work #1](https://github.com/xiaozhang-github/DST-Assessment-3/tree/main/Alex%20Caian)\n",
    "\n",
    "[Matt Corrie](https://github.com/mc17336) ~ [Work #2](https://github.com/xiaozhang-github/DST-Assessment-3/tree/main/Matt%20Corrie)\n",
    "\n",
    "[Xiao Zhang](https://github.com/xiaozhang-github) ~ [Work #3](https://github.com/xiaozhang-github/DST-Assessment-3/tree/main/Xiao%20Zhang)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
